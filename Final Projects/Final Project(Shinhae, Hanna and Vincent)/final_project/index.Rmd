---
title: "Final Project-An Analysis of Breast Cancer Data"
author:  "Hanna Kim, Shinhae Park, and Vincent Villalobos"
date: "Last updated on `r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
    df_print: kable
    code_folding: hide
    keep_md: true
---

This is a data analysis of the Breast Cancer Wisconsin (Diagnostic) Data Set ^[https://www.kaggle.com/uciml/breast-cancer-wisconsin-data]. This data set describes characteristics of the cell nuclei present in a digitized image. 

We restricted our analysis to every column, except standard error.

Here is the attribute Information:^[https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29]:

1) ID number 
2) Diagnosis (M = malignant, B = benign) 

Ten real-valued features are computed for each cell nucleus: 

a) radius (mean of distances from center to points on the perimeter) 
b) texture (standard deviation of gray-scale values) 
c) perimeter 
d) area 
e) smoothness (local variation in radius lengths) 
f) compactness (perimeter^2 / area - 1.0) 
g) concavity (severity of concave portions of the contour) 
h) concave points (number of concave portions of the contour) 
i) symmetry 
j) fractal dimension ("coastline approximation" - 1)

```{r include=FALSE}
library(tidyr)
library(dplyr)
library(DescTools) 
library(purrr)
library(grid)
library(knitr)
library(ggplot2)
source("http://peterhaschke.com/Code/multiplot.R") #load multiplot function
```


```{r echo = FALSE, include=FALSE}
setwd("/cloud/project/Day5/ProjectRelated/final_project/index_files/figure-html")
knitr::include_graphics("unnamed-chunk-11-1.png")
```


```{r, warning=FALSE, include=FALSE}
setwd("/cloud/project/Day5/ProjectRelated/final_project/data")
cancer<-read.csv("data.csv", header=FALSE)
```


```{r include=FALSE}
cancer2<-cancer %>% 
  select(2:13) %>% 
  drop_na() 
```


```{r include=FALSE}
colnames(cancer2)<-c("id", "diagnosis", "radius", "texture",
                     "perimeter", "area", "smoothness", "compactness",
                     "concavity", "concave points",
                     "symmetry","fractal_dimension")
#names(cancer2)
```


```{r echo = TRUE, include=FALSE}
table(cancer2$diagnosis)
```


# Regression Analysis

The number of people in each group who had diagnois as 'benign' or 'malignent'
Since response has binary values, 'Malignent'(equaling 1) and 'Benign'(equaling 0). We need to use logistic regression to see the data analysis. First, we ran logistic regression model separately on each of 10 variables.
Next, error rates which gauge the accuracy of our model, can be derived by logistic regression. If fitted value is bigger than 0.5, we predict all of data as 1. Likewise, if fitted value is smaller or equal than 0.5, we predict that the data as 0. 

If the predictions and the data do not match, we consider it as an error. The mean of all cases where two values do not match are called are error rate.

```{r echo = TRUE, include=FALSE}
# data cleaning
cancer2<-cancer2 %>%
  mutate(
    diagnosis = factor(diagnosis, 
                  levels = c("B", "M"),
                  labels = c(0, 1))
  )
```


```{r include=FALSE}
get_logit_plot= function(xval){
  ggplot(cancer2,aes(x=xval,y=as.numeric(diagnosis)-1))+
    geom_point()+
    geom_smooth( method="glm", method.args=list(family = "binomial"), se=FALSE)+
    ylab("diagnosis")
}
```


```{r include=FALSE}
get_error_rate=function(xval){
  pred <- ifelse(predict(glm(diagnosis~xval, family = "binomial", data=cancer2),type="response")>0.5,1,0)
  #table(cancer2$diagnosis,pred)
  error_rate=mean(cancer2$diagnosis!=pred)
  return(error_rate)
}
```


```{r include=FALSE}
plotlist<-cancer2 %>% map(get_logit_plot) 
#plotlist[3:12]
```


```{r include=FALSE}
plotlist2<-c()
j=1
for (i in names(plotlist[3:12])){
  p<-plotlist[[i]]+xlab(i)
  plotlist2[[j]]<-p
  j=j+1
}
```


```{r, echo=FALSE, cache=TRUE}
multiplot(plotlist = plotlist2, cols = 4)
#knitr::include_graphics("docs/plot_logits.png")
```

These are the logistic regressions on all of each variables. The blue line is the continuously predicted model of the data. The dots are the data points. 
As a Remark, the graph using fractal_dimension looks very different from other ones. We can see later also that it has the highest error rate. But if we use other ways to analyze the data, the conclusion has a twist.


```{r, warning=FALSE, include=FALSE}
err<-cancer2 %>% map(get_error_rate) 

j=1
Error=c()

for (i in names(err[3:12])){
  
  Error[j]<-err[[i]]
  j=j+1}

Error
```


```{r, echo=FALSE}
qplot(x=names(err[3:12]), y=Error, )+theme(axis.text.x=element_text(angle=45,hjust=1,vjust=1))+xlab("Variables")+ylab("Error Rates")
```



```{r, warning=FALSE}
logit_mod<-glm(diagnosis~area+ `concave points`+concavity+perimeter+radius, family = "binomial", data=cancer2)
summary(logit_mod)
```

```{r}
pred <- ifelse(predict(logit_mod,type="response")>0.5,1,0)
#table(cancer2$diagnosis,pred)
error_rate=mean(cancer2$diagnosis!=pred)
error_rate
```

If the error rate is bigger than the significant value, we drop such predictor variables. For example, if we only use only 5 out of 10 variables. The error rate is about 8%.

# Variable Selection

```{r, warning=FALSE}
fullmod=glm(diagnosis ~ .-id,family="binomial",data=cancer2)
summary(fullmod)
```

```{r}
pred <- ifelse(predict(fullmod,type="response")>0.5,1,0) #table(cancer2$diagnosis,pred) 
error_rate=mean(cancer2$diagnosis!=pred) 
error_rate
```

However, we saw that logistic regression with all variables has lower error rate. 
Therefore, we performed backward elimination of variables to select features. 

We choose a model by sequentially deleting one variable at a time according to a set of rules until a stopping criterion is met.

Step 1: Begin with the full model in all available variables.

Step 2: Remove the variable that has the lowest statistical
significance (smallest F-statistic or largest p-value). Continue
until the stopping rule is satisfied.

Stopping rule: All F values are greater than 3 (or every p-value is less than 0.05).

```{r, warning=FALSE, include=FALSE}
redmod=glm(diagnosis ~ .-id,family="binomial",data=cancer2)
drop1(redmod,test="F")
```

```{r, warning=FALSE, include=FALSE}
redmod<-update(redmod,. ~ . -compactness) 
drop1(redmod,test="F")
```


```{r, warning=FALSE, include=FALSE}
redmod<-update(redmod,. ~ . -perimeter)
drop1(redmod,test="F")
```

By the method of Backward Elimination, we dropped two variables `perimeter` and `compactness`.

```{r}
summary(redmod)
```

```{r}
redpred <- ifelse(predict(redmod,type="response")>0.5,1,0)
#table(cancer2$diagnosis,pred)
error_rate_red=mean(cancer2$diagnosis!=redpred)
error_rate_red
```

<!-- The error rate reduced by (around) 2.7% by adding `texture`, `smoothness`, `symmetry`, `fractal_dimension` and removing `perimeter`. -->
However, we observed that reduced model showed poor performance in prediction and goodness-of-fit than the full model.  

# Other Link 

We used other link functions to find the best model to predict the correct diagnosis of the breast cancer.

We first used all features as predictors. 

```{r, warning=FALSE, include=FALSE}
#probit 
full_probit<-glm(diagnosis~.-id, 
            family = binomial(link = probit), 
            data=cancer2)

#complimentary log log
full_loglog<-glm(diagnosis~.-id, 
            family = binomial(link =cloglog ), 
            data=cancer2)
```


```{r, include=FALSE}
pred_fullprobit <- ifelse(predict(full_probit,type="response")>0.5,1,0)
error_rate_fullprobit=mean(cancer2$diagnosis!=pred_fullprobit)
error_rate_fullprobit
```

```{r, include=FALSE}
pred_fullloglog <- ifelse(predict(full_loglog,type="response")>0.5,1,0)
error_rate_fullloglog=mean(cancer2$diagnosis!=pred_fullloglog)
error_rate_fullloglog
```

| Full model with Link | logit|probit|cloglog|
|:------|:--------|:--------|:--------|
| Error Rates | `r error_rate`  | `r error_rate_fullprobit` | `r error_rate_fullloglog`|
| Deviance | `r fullmod$deviance` | `r full_probit$deviance` | `r full_loglog$deviance` |
| PseudoR2 | `r PseudoR2(fullmod, c("McFadden"))` | `r PseudoR2(full_probit, c("McFadden"))` | `r PseudoR2(full_loglog, c("McFadden"))`|

Now, we used predictors chosen from the backward elimination. 

```{r, warning=FALSE, include=FALSE}
#probit 
probit<-glm(diagnosis~radius+texture+area+smoothness+`concave points`+concavity+symmetry+fractal_dimension, 
            family = binomial(link = probit), 
            data=cancer2)

#complimentary log log
loglog<-glm(diagnosis~radius+texture+area+smoothness+`concave points`+concavity+symmetry+fractal_dimension, 
            family = binomial(link =cloglog ), 
            data=cancer2)
```


```{r, include=FALSE}
pred_probit <- ifelse(predict(probit,type="response")>0.5,1,0)
error_rate_probit=mean(cancer2$diagnosis!=pred_probit)
error_rate_probit
```

```{r, include=FALSE}
pred_loglog <- ifelse(predict(loglog,type="response")>0.5,1,0)
error_rate_loglog=mean(cancer2$diagnosis!=pred_loglog)
error_rate_loglog
```


| Reduced model with Link | logit|probit|cloglog|
|:------|:--------|:--------|:--------|
| Error Rates | `r error_rate_red`  | `r error_rate_probit` | `r error_rate_loglog`|
| Deviance | `r redmod$deviance` | `r probit$deviance` | `r loglog$deviance` |
| PseudoR2 | `r PseudoR2(redmod, c("McFadden"))` | `r PseudoR2(probit, c("McFadden"))` | `r PseudoR2(loglog, c("McFadden"))`|

# Conclusion

We selected variables radius, texture, area, smoothness, `concave points`, concavity, symmetry, fractal_dimension
to predict if the breast cancer is malignant or benign.  
By the results of error rates for each link, logit or probit gave the more accurate prediction.   
To see the goodness-of-fit for generalized linear models, we further compared deviance residual and psuedo R sqaured. 
Using link cloglog on the variable 'diagnosis' had the smallest deviance residual. 
The largest pseudo R squared was obtained when we used loglog as link.
Therefore, if we focus on the prediction, either logit or probit are the best options. 
But, we will use cloglog to see the variablity of the data (almost 81% of the whole data can be exaplained by the model loglog). 